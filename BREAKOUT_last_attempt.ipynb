{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ***My Last Attempt at Training DQN for Breakout Before Using C51 Code***\n",
        "\n",
        "#The Previous Version of the Function that Generates Memory and Stores it\n",
        "This was the main problem as I see it. Generating data in this manner and storing it in a small buffer is inefficient.\n",
        "In the C51 code uploaded, the training starts once the buffer is filled with 80,000 global steps, and it's maximum size is 1,000,000 traisitions. This is a huge difference, and my naive implementation was far from approaching that magnitude.\n",
        "\n",
        "So, I'll keep this function here as part of the code to demonstrates the first attempt, and create a new function following it for actual future use.\n",
        "\n",
        "You can see in this version here how complex it was to implement this without the env wrappers. You can also see the debris of my logic.\n",
        "I tried to figure out what might make it tough for my neural network to learn the game and used that to make the early training easier.\n",
        "The problems I saw:\n",
        "\n",
        "\n",
        "1.   **Sparse Rewards**\n",
        "2.   **Inconsistent Actions**\n",
        "\n",
        "The sparse rewards and inconsistent actions could create uncertainty and disrupt the logic: what caused the reward to be achieved?\n",
        "\n",
        "The ideas I used to try and solve this:\n",
        "\n",
        "*   **Momentum**:\n",
        "\n",
        "    I figured using a momentum to determine the duration of each action could could address both issues. If at the beginig I set each action to be repeat itself for 25 steps - it might reduce uncertainty in actions leading to rewards, possibly helping the agent learn to be more consistent in its actions. The next part of the plan was to gradually decrease the number of action repetitions from 25 to 0, through out the training process.\n",
        "\n",
        "*   **Rewards Shaping**:\n",
        "\n",
        "    I figured using different reward function could improve the agent performance. I tried:\n",
        "\n",
        "      1.   Setting **future rewards** for actions that lead to rewards by exponentially increasing the rewards with each action until it reaches the actual reward. I experimented with different slopes, and tried applying this method exclusively to actions resulting in positive rewards.\n",
        "      2.   **Reward scaling**: I explored various approaches, such as setting reward=-1 for life lost or reward=-10 for life lost, etc.\n",
        "      3.   **Using only negetive reward**. Experimenting with the impact of sparse rewards and the number of steps taken from the meaningfull action itself to its outcome. I noticed negetive reward(life lost) is reached in fewer steps compared to the positive reward(hit brick). Therefor, I thought with fewer steps and Q value iterations representing them, the connection between not approaching the ball and losing life would be easier to establish. I wanted to focus on this and check if in fewer global steps the agent learns not to lose life. Additionally, at the beginning of the game, every time the paddle hits the ball without losing life, the ball hits some bricks. So, it made sense to me that this reward system could represent a good basic game strategy.\n",
        "      4.   Applying a **basic reward** for each step the agent stayed alive.\n",
        "      I tried giving the agent a basic reward for every step it stayed alive, inspired by my experience with 'cart-pole', where this worked well. In 'cart-pole,' with a basic reward of 1 for each step and a gamma value of 0.9, the Q values stabilized around Q ~ 10, making the system stable. I noticed the breakout agent didn't naturally aim for stable Q values like 'cart-pole' did, so I applied a similar idea to breakout. I gave a constant reward for staying alive (reward_alive=1), larger rewards for hitting bricks (reward_brick=5*reward), and set penalties for life lost (reward_lost=-5) with the next Q value at 0.\n",
        "\n",
        "\n",
        "\n",
        "Looking back, here are my thoughts about this process:\n",
        "\n",
        "*   **Micro managing**\n",
        "\n",
        "    I now realize that some of my ideas may be considered as micromanaging. What I find beautiful in the paper about DQN learning to play Atari is that the agent truly learns from the environment as it is. The original game's basic reward system, with no restrictions, no assistance, or mitigations. And the agent successfully learned the game. If the agent can learn on its own with minimum interference from our side, it is more valuable to me.\n",
        "\n",
        "*   **clumsy implementation**\n",
        "\n",
        "    The way I implemented the system in this code is somewhat clumsy. The training loop is not designed to sustain training over a memory buffer with a size of one million transitions. This implementation is essentially naive, vanilla, and not suitable for running training continuously for two days or more. While it works well for training a system on simpler problems like 'cart-pole,' it requires improvement to handle more extended training processes.\n",
        "\n",
        "*   **Other work**\n",
        "\n",
        "    Stepping up my work to match the latest solutions by actually using and integrating their code with mine is recommended. You can not make yourself available to solve new current problems, if you are too busy trying to solve past problems that took years for other people to solve. Harness what humanity has done so far, and use it to explore new horizons.\n",
        "\n",
        "*   **Future work**\n",
        "\n",
        "    If I had employed a better, more organized, and elegant code implementation like the one in the C51 code, I could have accurately observed and diagnosed the actual impact of the methods I used. Due to my clumsy implementation, it didn't lead to effective learning despite having the right logic behind it. In the future, I'm thinking of restructuring the code, running it with the suggested methods, and examining their effects on the agent's learning process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UkvEpN5EYwoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTS"
      ],
      "metadata": {
        "id": "qnLmAxpw6Yz-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQTAQD-AWZau",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "########################## IMPORTS #########################################\n",
        "%pip install gymnasium\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import time\n",
        "import cv2\n",
        "from IPython.display import clear_output\n",
        "%pip install \"gym[atari, accept-rom-license]\"\n",
        "import gym\n",
        "from google.colab.patches import cv2_imshow\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "\n",
        "!apt-get install -y python-opengl ffmpeg\n",
        "%pip install pyvirtualdisplay\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "from IPython.display import display, HTML\n",
        "from IPython.display import clear_output\n",
        "import io\n",
        "import base64\n",
        "import tensorflow as tf\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "USE GPU as device"
      ],
      "metadata": {
        "id": "yGim1J3A6bFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using the GPU\")\n",
        "\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "YwpNoYD0W23t",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build Neural Network"
      ],
      "metadata": {
        "id": "dQSZe-wxXQOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy_NN(nn.Module):        ############### Keep it simple ################\n",
        "    def __init__(self, n=8):\n",
        "        super(Policy_NN, self).__init__()\n",
        "        self.n_feature = n\n",
        "        self.kernel_size = 8\n",
        "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=self.kernel_size,stride=4)\n",
        "        self.conv2 = nn.Conv2d(32,64, kernel_size=self.kernel_size//2,stride=2 )\n",
        "        self.conv3 = nn.Conv2d(64,64, kernel_size=3,stride=1 )\n",
        "\n",
        "        self.fc1 = nn.Linear(64*7*7, 512)\n",
        "        self.fc2 = nn.Linear(512,256)\n",
        "        self.fc3 = nn.Linear(256,3)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.relu= nn.ReLU()\n",
        "        self.leaky_relu= nn.LeakyReLU(0.1)\n",
        "        self.log_softmax = nn.LogSoftmax( dim=1)\n",
        "        self.softmax = nn.Softmax( dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "                          # x size = (batch, n=4, 84, 84)\n",
        "        x = self.conv1(x) # x size = (batch, 12, 84, 84)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x) # x size = (batch, 12, 42, 42)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x) # x size = (batch, 6, 21, 21)\n",
        "\n",
        "        x = self.conv3(x) # x size = (batch, 3, 21, 21)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)  # x size = (batch, 3, 10, 10)\n",
        "\n",
        "        batch_size, _, height, width = x.size()\n",
        "\n",
        "        x = x.reshape(batch_size, -1)\n",
        "\n",
        "\n",
        "##### FULLY CONNECTED LAYERS\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ad-KicqNgHTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Images Function\n",
        "turn then to grayscale and resize them to (84,84)"
      ],
      "metadata": {
        "id": "Utz5cPCTXzty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(rgb_image):\n",
        "    # Convert the RGB image to grayscale\n",
        "    grayscale_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Resize the grayscale image to 84x84\n",
        "    resized_image = cv2.resize(grayscale_image, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Set a threshold value\n",
        "    threshold_value = 10  # You can adjust this threshold as needed\n",
        "\n",
        "    # Apply thresholding to create a black and white image\n",
        "    _, black_and_white_image = cv2.threshold(resized_image, threshold_value, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    return black_and_white_image\n",
        "\n",
        "def preproces_mini_batch(last4states):  #turn to 10 states\n",
        "    index = 0\n",
        "    img0 = np.array(preprocess_image(last4states[index]))\n",
        "    img1 = np.array(preprocess_image(last4states[index+1]))\n",
        "    img2 = np.array(preprocess_image(last4states[index+2]))\n",
        "    img3 = np.array(preprocess_image(last4states[index+3]))\n",
        "    stacked_state = np.stack((img0,img1,img2,img3),axis =2)\n",
        "    return stacked_state"
      ],
      "metadata": {
        "id": "hbxJnEvhXv3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build an OFFLINE_gradient_descent function**"
      ],
      "metadata": {
        "id": "jqEJ1fYmB26H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def OFFLINE_gradient_descent(model, memory, batch_size, learning_rate, gamma=0.9):\n",
        "    model_copy = copy.deepcopy(model).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    len_memory = len(memory)\n",
        "    print(\"len_memory=\", len_memory, \"batch_size=\", batch_size)\n",
        "    ## Can add  random.shuffle(memory)\n",
        "    for batch_start in range(0, len_memory, batch_size):\n",
        "        batch_memory = memory[batch_start:batch_start + batch_size]\n",
        "\n",
        "        # Unpack batch_memory\n",
        "        States, Actions, Rewards, Next_States, Future_Rewards, Lost_Ball = zip(*batch_memory)\n",
        "\n",
        "        # Convert to tensors\n",
        "        States = torch.Tensor(States).transpose(1, 3).transpose(2,3) #from [n,84,84,4] to [n,4,84,84]\n",
        "        Actions = torch.tensor(Actions, dtype=int).to(device)\n",
        "        Rewards = torch.tensor(Rewards, dtype=torch.float32).to(device)\n",
        "        Next_States = torch.Tensor(Next_States).transpose(1, 3).transpose(2,3) #from [n,84,84,4] to [n,4,84,84]\n",
        "        Future_Rewards = torch.tensor(Future_Rewards,dtype=torch.float32).to(device)\n",
        "        Lost_Ball = torch.tensor(Lost_Ball,dtype=int).to(device)\n",
        "\n",
        "        # Get Q-values for current states\n",
        "        Q_values = model(States)\n",
        "\n",
        "        # Compute targets\n",
        "        with torch.no_grad():\n",
        "            max_next_Q_values, _ = torch.max(model_copy(Next_States), dim=1, keepdim=True)\n",
        "            #print(\"max_next_Q_values=\",max_next_Q_values) ## Sanity check\n",
        "            #print(\"Lost_Ball.int()=\",Lost_Ball.int())  ## Sanity check\n",
        "            #print(\"Rewards=\", Rewards)  ## Sanity check\n",
        "            targets = 10*Rewards + Lost_Ball.int() + gamma * (Lost_Ball.int()) * max_next_Q_values.squeeze(dim=1) ### I did not use the Future Rewards option\n",
        "            #print(\"Future =\", Future_Rewards) ## Sanity check\n",
        "            #print(\"targets=\",targets) ## Sanity check\n",
        "            targets = targets.reshape(-1,1)\n",
        "            Future_Rewards = Future_Rewards.reshape(-1,1)\n",
        "            #print(\"targets after reshape=\",targets) ## Sanity check\n",
        "            #targets = targets + Future_Rewards\n",
        "            #print(\"targets after reshape=\",targets) ## Sanity check\n",
        "            targets = targets.to(device)\n",
        "            #targets = 10*torch.ones_like(targets, dtype=torch.float32) ######### Sanity check ####### Make sure to cancel !!!!!!\n",
        "\n",
        "        # Zero-out Q-values for actions not taken\n",
        "        Q_values_selected = torch.gather(Q_values, 1, Actions.view(-1, 1))\n",
        "        #print(\"Q_values_selected=\",Q_values_selected) ## Sanity check\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(targets,Q_values_selected)\n",
        "        #print(\"Q_values_selected=\", Q_values_selected) ## Sanity check\n",
        "        #print(\"diff =\",targets - Q_values_selected ) ## Sanity check\n",
        "\n",
        "        # Perform optimization step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_start % 50 == 0: model_copy = copy.deepcopy(model).to(device)  # Update target network\n",
        "        # Print loss\n",
        "        if batch_start % 50 == 0:\n",
        "            print(\"----------------------------------------------------\")\n",
        "            print(\"batch=\", batch_start, \"loss=\", float(loss.item()))\n",
        "            print(\"Q_values_selected[-1]=\",Q_values_selected[-1].item(),\"targets[-1]=\",targets[-1].item(),\"max_next_Q_values[-1]=\",max_next_Q_values[-1].item())\n",
        "\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "svXJLIOU5yjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the NN Argument"
      ],
      "metadata": {
        "id": "kuPxkWXWCHMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "police = Policy_NN()\n",
        "#police.load_state_dict(torch.load('/content/gdrive/My Drive/DeepLearning/BreakOut/ckpt-100.pk'))\n",
        "police = police.to(device)"
      ],
      "metadata": {
        "id": "JXcmvRDMCFx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Previous Version of the Function that Generates and Stores Memory"
      ],
      "metadata": {
        "id": "nQvfAjA05l3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_choose(model,stacked_state):\n",
        "      xt = np.array([stacked_state])          ### Enter last 4 states stacked together\n",
        "      XT = torch.Tensor(xt).transpose(1, 3).transpose(2,3) ### Turn to Tensor and transpose to (1,4,84,84)\n",
        "      XT = XT.to(device)\n",
        "      ZS = model(XT)\n",
        "      if torch.all(ZS == ZS[0]).item(): ### In case it devides equally, select randomly\n",
        "          action = random.randint(0, 2)\n",
        "      else : action = torch.argmax(ZS).item()\n",
        "      return action\n",
        "\n",
        "def print_and_track(printed_content,*args, sep=' ', end='\\n'):\n",
        "    content = sep.join(map(str, args)) + end\n",
        "    printed_content.append(content)\n",
        "    print(content, end='')\n",
        "\n",
        "def play_states(states,sec):\n",
        "    states = np.array(states)\n",
        "    for i in range(states.shape[0]):\n",
        "        cv2_imshow(states[i])\n",
        "        time.sleep(sec)\n",
        "        clear_output(wait=True)"
      ],
      "metadata": {
        "id": "C1_8laF8jhSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def Create_Data(model,Explore,length,Epsilon,len_action,Force_good=True):\n",
        "    printed_content = []\n",
        "    env = gym.make(\"BreakoutNoFrameskip-v4\") ### Create enviroment\n",
        "    reset_state = env.reset()\n",
        "    Good_Exp, Good_states = [],[]\n",
        "    for episode in range(50):\n",
        "        Momentum, life, count, lost_ball, step = 0, 5, 0, 1, 0\n",
        "        print_and_track(printed_content,\"new episode, step =\", step)\n",
        "        Exp, states, episode_rewards, episode_rewards_pos = [],[],[], []\n",
        "        states.append(reset_state)\n",
        "        if len(Good_Exp) > length :\n",
        "            print_and_track(printed_content,\"long enough\")\n",
        "            break ### enough\n",
        "\n",
        "        while step < 2000:\n",
        "            if step < 3 :\n",
        "                print_and_track(printed_content,\"step =\", step)\n",
        "                action = 0\n",
        "            else :\n",
        "                stacked_state = preproces_mini_batch(states[-4:])\n",
        "                if Momentum >= len_action or step == 3 :\n",
        "                    ### Choose new action\n",
        "                    Momentum = 0\n",
        "                    ### Choose explore or exploite\n",
        "                    exploration_rate_threshold = random.uniform(0, 1)\n",
        "                    if exploration_rate_threshold < Epsilon  :\n",
        "                        #action = random.randint(1, 2)\n",
        "                        action = random.randint(1, 2)\n",
        "                        print_and_track(printed_content,\"Random action\")\n",
        "                    else :\n",
        "                        print_and_track(printed_content,\"model choose\")\n",
        "                        action = model_choose(model,stacked_state)\n",
        "            if lost_ball == 0 :\n",
        "                action = 0 ## CHECK again\n",
        "                lost_ball = 1 ## Should I change step to step = 0 ?\n",
        "\n",
        "            state, reward, done, info = env.step(action+1)\n",
        "            Momentum += 1 ## CHECK again\n",
        "            count +=1\n",
        "            states.append(state)\n",
        "            episode_rewards_pos.append(reward)\n",
        "            if info['lives'] < life :\n",
        "                reward = -1\n",
        "                lost_ball = 0\n",
        "                Momentum = 0 #len_action - 1\n",
        "                print_and_track(printed_content,\"life=\",info['lives'],\"compare_life=\",life,\"break for lost ball\")\n",
        "            episode_rewards.append(reward)\n",
        "            ### ADD to memory\n",
        "            if count == 3 and len(Exp) > 0 :\n",
        "                print_and_track(printed_content,\"add next states in step =\", step)\n",
        "                if Exp[-1][1] == 0 : print(\"FIRE\")\n",
        "                if Exp[-1][1] == 1 : print(\"RIGHT\")\n",
        "                if Exp[-1][1] == 2 : print(\"LEFT\")\n",
        "                time.sleep(2)\n",
        "                clear_output(wait=True)\n",
        "                play_states(states[-4:],1)\n",
        "                Exp[-1][3] = stacked_state\n",
        "                new_momentum_reward = sum(episode_rewards[-4:])  ### Rewrite Past reward, because it may gained reward while 0<count<3\n",
        "                Exp[-1][2] = new_momentum_reward\n",
        "                #### Give Human Feedback\n",
        "                User_Reward = input('What is the Reward? Reward in[-1,1]\\n')\n",
        "                print(\"User_Reward=\", User_Reward)\n",
        "                Exp[-1][4] = float(User_Reward)\n",
        "            if ((Momentum%4 == 0 or lost_ball == 0) and step > 3) or step == 3 :\n",
        "                count = 0 ## 4 Frames count\n",
        "                stacked_next_state = preproces_mini_batch(states[-4:])\n",
        "                print_and_track(printed_content,\"add exp in step =\", step)\n",
        "                time.sleep(2)\n",
        "                clear_output(wait=True)\n",
        "                play_states(states[-4:],0.5)\n",
        "                Exp.append([stacked_state, action, reward, stacked_next_state, 0,lost_ball])\n",
        "                print_and_track(printed_content,\"action = \", action)\n",
        "\n",
        "            if done == True or lost_ball == 0 :\n",
        "                reset_state = env.reset()\n",
        "                step = length\n",
        "                print_and_track(printed_content,\"Done\")\n",
        "                break ### No more steps for this episode\n",
        "            step += 1\n",
        "        time.sleep(3)\n",
        "        clear_output(wait=True)\n",
        "        play_states(states,0.05)\n",
        "        for item in printed_content:\n",
        "            print(item, end='')\n",
        "        total_reward_per_episode = sum(episode_rewards_pos)\n",
        "        if Force_good == False :\n",
        "            print_and_track(printed_content,\"got it\")\n",
        "            Good_states.extend(states)\n",
        "            Good_Exp.extend(Exp)\n",
        "        elif total_reward_per_episode > 0.5 and Force_good == True :\n",
        "            print_and_track(printed_content,\"got it\")\n",
        "            Good_states.extend(states)\n",
        "            Good_Exp.extend(Exp)\n",
        "    env.close()                                            ### Close env\n",
        "    return Good_Exp, Good_states"
      ],
      "metadata": {
        "id": "6s9134DBjlph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Create_Data_no_print(model,Explore,length,Epsilon,len_action,Force_good=True):\n",
        "    printed_content = []\n",
        "    env = gym.make(\"BreakoutNoFrameskip-v4\") ### Create enviroment\n",
        "    reset_state = env.reset()\n",
        "    Good_Exp, Good_states = [],[]\n",
        "    for episode in range(50):\n",
        "        Momentum, life, count, lost_ball, step = 0, 5, 0, 1, 0\n",
        "        #print_and_track(printed_content,\"new episode, step =\", step)\n",
        "        Exp, states, episode_rewards, episode_rewards_pos = [],[],[], []\n",
        "        states.append(reset_state)\n",
        "        if len(Good_Exp) > length :\n",
        "            #print_and_track(printed_content,\"long enough\")\n",
        "            break ### enough\n",
        "\n",
        "        while step < 2000:\n",
        "            if step < 3 :\n",
        "                #print_and_track(printed_content,\"step =\", step)\n",
        "                action = 0\n",
        "            else :\n",
        "                stacked_state = preproces_mini_batch(states[-4:])\n",
        "                if Momentum >= len_action or step == 3 :\n",
        "                    ### Choose new action\n",
        "                    Momentum = 0\n",
        "                    ### Choose explore or exploite\n",
        "                    exploration_rate_threshold = random.uniform(0, 1)\n",
        "                    if exploration_rate_threshold < Epsilon  :\n",
        "                        #action = random.randint(1, 2)\n",
        "                        action = random.randint(1, 2)\n",
        "                        #print_and_track(printed_content,\"Random action\")\n",
        "                    else :\n",
        "                        #print_and_track(printed_content,\"model choose\")\n",
        "                        action = model_choose(model,stacked_state)\n",
        "            if lost_ball == 0 :\n",
        "                action = 0 ## CHECK again\n",
        "                lost_ball = 1 ## Should I change step to step = 0 ?\n",
        "\n",
        "            state, reward, done, info = env.step(action+1)\n",
        "            Momentum += 1 ## CHECK again\n",
        "            count +=1\n",
        "            states.append(state)\n",
        "            episode_rewards_pos.append(reward)\n",
        "            if info['lives'] < life :\n",
        "                reward = -1\n",
        "                lost_ball = 0\n",
        "                Momentum = 0 #len_action - 1\n",
        "                #print_and_track(printed_content,\"life=\",info['lives'],\"compare_life=\",life,\"break for lost ball\")\n",
        "            episode_rewards.append(reward)\n",
        "            ### ADD to memory\n",
        "            if count == 3 and len(Exp) > 0 :\n",
        "                #print_and_track(printed_content,\"add next states in step =\", step)\n",
        "                '''if Exp[-1][1] == 0 : print(\"FIRE\")\n",
        "                if Exp[-1][1] == 1 : print(\"RIGHT\")\n",
        "                if Exp[-1][1] == 2 : print(\"LEFT\")\n",
        "                time.sleep(2)\n",
        "                clear_output(wait=True)\n",
        "                play_states(states[-4:],1)'''\n",
        "                Exp[-1][3] = stacked_state\n",
        "                new_momentum_reward = sum(episode_rewards[-4:])  ### Rewrite Past reward, because it may gained reward while 0<count<3\n",
        "                Exp[-1][2] = new_momentum_reward\n",
        "                #### Give Human Feedback\n",
        "                # Nope.\n",
        "            if ((Momentum%4 == 0 or lost_ball == 0) and step > 3) or step == 3 :\n",
        "                count = 0 ## 4 Frames count\n",
        "                stacked_next_state = preproces_mini_batch(states[-4:])\n",
        "                '''print_and_track(printed_content,\"add exp in step =\", step)\n",
        "                time.sleep(2)\n",
        "                clear_output(wait=True)\n",
        "                play_states(states[-4:],0.5)'''\n",
        "                Exp.append([stacked_state, action, reward, stacked_next_state, 0,lost_ball])\n",
        "                #print_and_track(printed_content,\"action = \", action)\n",
        "\n",
        "            if done == True or lost_ball == 0 :\n",
        "                reset_state = env.reset()\n",
        "                step = length\n",
        "                #print_and_track(printed_content,\"Done\")\n",
        "                break ### No more steps for this episode\n",
        "            step += 1\n",
        "        #time.sleep(3)\n",
        "        #clear_output(wait=True)\n",
        "       #play_states(states,0.01)\n",
        "        for item in printed_content:\n",
        "            print(item, end='')\n",
        "        total_reward_per_episode = sum(episode_rewards_pos)\n",
        "        if Force_good == False :\n",
        "            #print_and_track(printed_content,\"got it\")\n",
        "            Good_states.extend(states)\n",
        "            Good_Exp.extend(Exp)\n",
        "        elif total_reward_per_episode > 0.5 and Force_good == True :\n",
        "            #print_and_track(printed_content,\"got it\",\"reward=\",total_reward_per_episode)\n",
        "            Good_states.extend(states)\n",
        "            Good_Exp.extend(Exp)\n",
        "    env.close()                                            ### Close env\n",
        "    return Good_Exp, Good_states"
      ],
      "metadata": {
        "id": "kr-P-Whb3aMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_Exp, G_states = Create_Data_no_print(police,True,600,1,25,Force_good=True)"
      ],
      "metadata": {
        "id": "1TmIyaQY56W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The New Version of the Function that Generates and Stores Memory\n",
        "In the new version I am going to be efficient and use the atari env wrappers and a large buffer with max size 1,000,000."
      ],
      "metadata": {
        "id": "uD6cC-hq_WKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### CHECKOUT NP SAVE\n",
        "#G_Exp = np.array(G_Exp)\n",
        "#np.save('nice_exp.npy',G_Exp)\n",
        "#arr_loaded = np.load('nice_exp.npy', allow_pickle= True)"
      ],
      "metadata": {
        "id": "UpVL3w1oCLvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Start Training***"
      ],
      "metadata": {
        "id": "z8epgxV3EhOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch =10\n",
        "m=0.0001\n",
        "\n",
        "OFFLINE_gradient_descent(police,G_Exp, batch_size=batch, learning_rate=m, gamma=0.9)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "HSS1ekG0EgHU",
        "outputId": "1083bdc7-d03b-4f5c-900f-0bea69531781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len_memory= 493 batch_size= 10\n",
            "----------------------------------------------------\n",
            "batch= 0 loss= 10.67123031616211\n",
            "Q_values_selected[-1]= -1.6344971656799316 targets[-1]= 1.4943656921386719 max_next_Q_values[-1]= 0.54929518699646\n",
            "----------------------------------------------------\n",
            "batch= 50 loss= 0.8541105389595032\n",
            "Q_values_selected[-1]= 1.8050105571746826 targets[-1]= 1.4838006496429443 max_next_Q_values[-1]= 0.5375563502311707\n",
            "----------------------------------------------------\n",
            "batch= 100 loss= 2.7141571044921875\n",
            "Q_values_selected[-1]= 3.615630626678467 targets[-1]= 2.9711811542510986 max_next_Q_values[-1]= 2.1902012825012207\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-6af10a75d6cd>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mOFFLINE_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG_Exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-81ecdae367ac>\u001b[0m in \u001b[0;36mOFFLINE_gradient_descent\u001b[0;34m(model, memory, batch_size, learning_rate, gamma)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mActions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mRewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mNext_States\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNext_States\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#from [n,84,84,4] to [n,4,84,84]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m#Next_States = Next_States.transpose(1, 3).transpose(2,3) #from [n,84,84,4] to [n,4,84,84]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mFuture_Rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFuture_Rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Nice_EXP = []\n",
        "police = Policy_NN()"
      ],
      "metadata": {
        "id": "x7z_01BJEI7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Nice_EXP = []"
      ],
      "metadata": {
        "id": "U8kEOOO-ZzQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch =10\n",
        "m=0.00000001\n",
        "epsilon = 0.95\n",
        "K = 25\n",
        "for epoch in range(100):\n",
        "\n",
        "    if epoch%5 == 0 and epoch!=0 : m=m/10  ##Create_Data_no_print(model,Explore,length,Epsilon,len_action,Force_good=True)\n",
        "    print(\"*******************************************************\\n*******************************************************\")\n",
        "    print(\"Epoch=\",epoch, \"LR=\",m,\"epsilon=\",epsilon)\n",
        "    if epoch%5 == 0 :\n",
        "      Nice_EXP = []\n",
        "      New_G_Exp, New_G_states = Create_Data_no_print(police,True,60,epsilon,25,Force_good=True)\n",
        "      Nice_EXP.extend(New_G_Exp)\n",
        "    epsilon = 0.99*epsilon\n",
        "    for inner_epoch in range(10):\n",
        "        OFFLINE_gradient_descent(police, Nice_EXP, batch_size=batch, learning_rate=m, gamma=0.9)"
      ],
      "metadata": {
        "id": "4ZIdS2UI6D9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def creat_video(model,name,Explore,Force_goodnes = False, length = 100,Th = 0.5,K=25,Use = True):\n",
        "    print(\"Explore =\", Explore)        ######create_more_data(model,Force_good = True, Explore = True, length = 600, Epsilon = 1,Big_Momentum_TH=25,use = True)\n",
        "    #Memory,states_tryout,rewardss, avg_rrs = create_more_data(model,Force_goodnes, Explore, length = 100,Epsilon = Th,Big_Momentum_TH=K, use = Use )\n",
        "    Memory , states_tryout = Create_Data_no_print(police,True,length,0,25,Force_good=Force_goodnes)\n",
        "    #Rewards = np.array([item[2] for item in Memory])\n",
        "    #Actions = np.array([item[1] for item in Memory])\n",
        "    #Future = np.array([item[4] for item in Memory])\n",
        "    #Sum = np.array([item[6] for item in Memory])\n",
        "    #print(\"Rewards=\",Rewards)\n",
        "    #print(\"Actions =\", Actions)\n",
        "    #print(\"Future=\",Future)\n",
        "    print(\"Avg =\",avg_rrs)\n",
        "\n",
        "    image_arrays = np.array(states_tryout)\n",
        "\n",
        "    # Define the video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "    video_writer = cv2.VideoWriter('output_video_{}.mp4'.format(name), fourcc, 60.0, (image_arrays[0].shape[1], image_arrays[0].shape[0]))\n",
        "\n",
        "    # Write each frame to the video file\n",
        "    for image_array in image_arrays:\n",
        "        video_writer.write(image_array)\n",
        "\n",
        "    # Release the video writer\n",
        "    video_writer.release()\n",
        "\n",
        "    # Display the video in Colab\n",
        "    video_path = 'output_video_{}.mp4'.format(name)\n",
        "    video_file = io.open(video_path, 'r+b').read()\n",
        "    encoded = base64.b64encode(video_file)\n",
        "    HTML(data='''<video alt=\"test\" controls>\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(encoded.decode('ascii')))\n",
        "    return Memory,states_tryout,rewardss, avg_rrs\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def delete_videos_by_name(name_pattern):\n",
        "    video_files = glob.glob(f'output_video_{name_pattern}.mp4')\n",
        "\n",
        "    for video_file in video_files:\n",
        "        try:\n",
        "            os.remove(video_file)\n",
        "            print(f\"Deleted: {video_file}\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error deleting {video_file}: {e}\")\n",
        "\n",
        "# Replace 'your_name_pattern' with the actual name pattern you want to delete\n",
        "delete_videos_by_name('your_name_pattern')"
      ],
      "metadata": {
        "id": "z92TWcOUyl0f",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "Memory,states_tryout,rewardss, avg_rrs = creat_video(police,202, False,False,100,0,25,Use =0)"
      ],
      "metadata": {
        "id": "H1tF2NkPy29c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}